{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "iCGzcrgNquk6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "7paJP_A67rxD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Utilize VGG16 for Feature Extraction\n",
        "# Load VGG16 model pre-trained on ImageNet, excluding the top layers\n",
        "vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
        "\n",
        "# Freeze the convolutional layers of VGG16\n",
        "for layer in vgg_base.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "ivNiYTOh3qVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9fe6d37-4393-4eb8-be88-2d8178feeb14"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('archive.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/extracted_images')"
      ],
      "metadata": {
        "id": "E8t42l7evXeU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Data Preprocessing and Augmentation\n",
        "# Setup data generators for training and validation\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/extracted_images',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    '/content/extracted_images',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN10I36b3jn4",
        "outputId": "b6625433-8d9c-4e57-c79d-b08769254b1f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 images belonging to 2 classes.\n",
            "Found 25000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Build and Train the Classification Model\n",
        "# Add custom fully connected layers on top of the VGG16 base model\n",
        "model = models.Sequential()\n",
        "\n",
        "# Ensure you are using only one output from vgg_base.\n",
        "# If vgg_base has multiple outputs, select the appropriate one using vgg_base.output[index]\n",
        "model.add(vgg_base)\n",
        "\n",
        "# Add the Flatten layer\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "JJTLpGpE3g_I"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Model Compilation and Training\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizers.SGD(learning_rate=0.001, momentum=0.9),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=25,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOdlAlbp3c-D",
        "outputId": "5a551f31-1268-4db2-875e-d149af022a3e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 174ms/step - accuracy: 0.5746 - loss: 0.7112 - val_accuracy: 0.8180 - val_loss: 0.4379\n",
            "Epoch 2/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 171ms/step - accuracy: 0.7082 - loss: 0.5539 - val_accuracy: 0.8460 - val_loss: 0.4054\n",
            "Epoch 3/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 163ms/step - accuracy: 0.7511 - loss: 0.5134 - val_accuracy: 0.8450 - val_loss: 0.3715\n",
            "Epoch 4/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 174ms/step - accuracy: 0.7560 - loss: 0.5102 - val_accuracy: 0.7140 - val_loss: 0.5300\n",
            "Epoch 5/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 164ms/step - accuracy: 0.7399 - loss: 0.5133 - val_accuracy: 0.8250 - val_loss: 0.3803\n",
            "Epoch 6/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 164ms/step - accuracy: 0.7500 - loss: 0.5022 - val_accuracy: 0.8600 - val_loss: 0.3491\n",
            "Epoch 7/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 162ms/step - accuracy: 0.7788 - loss: 0.4617 - val_accuracy: 0.8650 - val_loss: 0.3252\n",
            "Epoch 8/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 161ms/step - accuracy: 0.7903 - loss: 0.4406 - val_accuracy: 0.8320 - val_loss: 0.3563\n",
            "Epoch 9/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 163ms/step - accuracy: 0.7898 - loss: 0.4458 - val_accuracy: 0.8650 - val_loss: 0.3064\n",
            "Epoch 10/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 169ms/step - accuracy: 0.8073 - loss: 0.4417 - val_accuracy: 0.8330 - val_loss: 0.3617\n",
            "Epoch 11/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 161ms/step - accuracy: 0.7847 - loss: 0.4595 - val_accuracy: 0.8820 - val_loss: 0.2873\n",
            "Epoch 12/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 160ms/step - accuracy: 0.8096 - loss: 0.4145 - val_accuracy: 0.8430 - val_loss: 0.3444\n",
            "Epoch 13/25\n",
            "\u001b[1m 50/100\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 122ms/step - accuracy: 0.7659 - loss: 0.4694"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 88ms/step - accuracy: 0.7750 - loss: 0.4613 - val_accuracy: 0.8770 - val_loss: 0.3099\n",
            "Epoch 14/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 168ms/step - accuracy: 0.8048 - loss: 0.4167 - val_accuracy: 0.8810 - val_loss: 0.2876\n",
            "Epoch 15/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 168ms/step - accuracy: 0.8025 - loss: 0.4082 - val_accuracy: 0.8920 - val_loss: 0.2715\n",
            "Epoch 16/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 168ms/step - accuracy: 0.8092 - loss: 0.4127 - val_accuracy: 0.8700 - val_loss: 0.3177\n",
            "Epoch 17/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 163ms/step - accuracy: 0.8086 - loss: 0.4282 - val_accuracy: 0.8580 - val_loss: 0.3149\n",
            "Epoch 18/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 170ms/step - accuracy: 0.8004 - loss: 0.4239 - val_accuracy: 0.8630 - val_loss: 0.2910\n",
            "Epoch 19/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 166ms/step - accuracy: 0.7760 - loss: 0.4482 - val_accuracy: 0.8740 - val_loss: 0.2849\n",
            "Epoch 20/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 163ms/step - accuracy: 0.7947 - loss: 0.4220 - val_accuracy: 0.8920 - val_loss: 0.2661\n",
            "Epoch 21/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 161ms/step - accuracy: 0.8228 - loss: 0.3857 - val_accuracy: 0.8710 - val_loss: 0.3059\n",
            "Epoch 22/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 162ms/step - accuracy: 0.8330 - loss: 0.3744 - val_accuracy: 0.8970 - val_loss: 0.2517\n",
            "Epoch 23/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 168ms/step - accuracy: 0.8260 - loss: 0.3866 - val_accuracy: 0.8780 - val_loss: 0.2865\n",
            "Epoch 24/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 163ms/step - accuracy: 0.7939 - loss: 0.4348 - val_accuracy: 0.8880 - val_loss: 0.2548\n",
            "Epoch 25/25\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 159ms/step - accuracy: 0.8132 - loss: 0.3970 - val_accuracy: 0.8900 - val_loss: 0.2692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Evaluate and Test the Model\n",
        "# Evaluate the model on the validation data\n",
        "val_loss, val_acc = model.evaluate(validation_generator, steps=50)\n",
        "print(f'Validation accuracy: {val_acc}')"
      ],
      "metadata": {
        "id": "xJF0jne-3W7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10e573f1-acea-42f4-c1a6-e1dc618cac9c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step - accuracy: 0.9043 - loss: 0.2751\n",
            "Validation accuracy: 0.8939999938011169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('my_model.keras')"
      ],
      "metadata": {
        "id": "7SJzAwRR3ZlC"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def predict_image(model, image_path):\n",
        "    \"\"\"\n",
        "    Predict the class of a new image using the trained model.\n",
        "\n",
        "    Parameters:\n",
        "    model: The trained Keras model used for prediction.\n",
        "    image_path: Path to the image file to be classified.\n",
        "\n",
        "    Returns:\n",
        "    Prints the predicted category and the confidence level.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load and preprocess the image\n",
        "        img = image.load_img(image_path, target_size=(150, 150))  # Adjust target size to your model's input size\n",
        "        img_tensor = image.img_to_array(img)  # Convert image to numpy array\n",
        "        img_tensor = np.expand_dims(img_tensor, axis=0)  # Add batch dimension\n",
        "        img_tensor /= 255.0  # Normalize pixel values to [0, 1]\n",
        "\n",
        "        # Make prediction\n",
        "        prediction = model.predict(img_tensor)\n",
        "        confidence = prediction[0][0]  # Get the confidence score (since it's a binary classification)\n",
        "\n",
        "        # Determine the predicted class and confidence level\n",
        "        if confidence > 0.5:\n",
        "            predicted_label = 'Dog'\n",
        "            confidence_level = confidence\n",
        "        else:\n",
        "            predicted_label = 'Cat'\n",
        "            confidence_level = 1 - confidence  # Confidence for Cat is (1 - confidence)\n",
        "\n",
        "        # Print the result\n",
        "        print(f\"Predicted category: {predicted_label}\")\n",
        "        print(f\"Confidence level: {confidence_level:.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "xNcnzl40mPJv"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_image(model, '/content/extracted_images/cats/cat.1.jpg'))\n",
        "\n"
      ],
      "metadata": {
        "id": "ryKqZKm_HM0b",
        "outputId": "c6e3c4c8-e634-4383-b52a-56bcb63728f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
            "Predicted category: Cat\n",
            "Confidence level: 1.00\n",
            "None\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}